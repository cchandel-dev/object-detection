{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc449fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.17\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "Package                 Version\n",
      "----------------------- ---------\n",
      "absl-py                 0.15.0\n",
      "anyio                   3.5.0\n",
      "appdirs                 1.4.4\n",
      "argon2-cffi             21.3.0\n",
      "argon2-cffi-bindings    21.2.0\n",
      "asttokens               2.0.5\n",
      "astunparse              1.6.3\n",
      "attrs                   22.1.0\n",
      "Babel                   2.11.0\n",
      "backcall                0.2.0\n",
      "basemap-data            1.3.2\n",
      "bayesian-optimization   1.4.3\n",
      "bleach                  4.1.0\n",
      "boto3                   1.24.28\n",
      "botocore                1.27.59\n",
      "Bottleneck              1.3.5\n",
      "brotlipy                0.7.0\n",
      "cachetools              5.3.1\n",
      "certifi                 2023.7.22\n",
      "cffi                    1.15.1\n",
      "charset-normalizer      2.0.4\n",
      "click                   8.0.4\n",
      "cloudpickle             2.2.1\n",
      "colorama                0.4.6\n",
      "comm                    0.1.2\n",
      "contourpy               1.0.5\n",
      "cryptography            41.0.2\n",
      "cycler                  0.11.0\n",
      "debugpy                 1.5.1\n",
      "decorator               5.1.1\n",
      "defusedxml              0.7.1\n",
      "entrypoints             0.4\n",
      "executing               0.8.3\n",
      "fastjsonschema          2.16.2\n",
      "Flask                   2.2.2\n",
      "flatbuffers             1.12\n",
      "fonttools               4.25.0\n",
      "gast                    0.3.3\n",
      "google-auth             2.22.0\n",
      "google-auth-oauthlib    0.4.6\n",
      "google-pasta            0.2.0\n",
      "grpcio                  1.32.0\n",
      "gym                     0.26.2\n",
      "gym-notices             0.0.8\n",
      "h5py                    2.10.0\n",
      "idna                    3.4\n",
      "importlib-metadata      6.0.0\n",
      "importlib-resources     5.2.0\n",
      "ipykernel               6.19.2\n",
      "ipython                 8.12.0\n",
      "ipython-genutils        0.2.0\n",
      "ipywidgets              8.0.4\n",
      "itsdangerous            2.0.1\n",
      "jedi                    0.18.1\n",
      "Jinja2                  3.1.2\n",
      "jmespath                0.10.0\n",
      "joblib                  1.2.0\n",
      "json5                   0.9.6\n",
      "jsonschema              4.17.3\n",
      "jupyter                 1.0.0\n",
      "jupyter_client          8.1.0\n",
      "jupyter-console         6.6.3\n",
      "jupyter_core            5.3.0\n",
      "jupyter-server          1.13.5\n",
      "jupyterlab              3.3.2\n",
      "jupyterlab-pygments     0.2.2\n",
      "jupyterlab_server       2.16.3\n",
      "jupyterlab-widgets      3.0.5\n",
      "kaggle                  1.5.16\n",
      "Keras-Preprocessing     1.1.2\n",
      "keras-tuner             1.3.5\n",
      "kiwisolver              1.4.4\n",
      "kt-legacy               1.0.5\n",
      "lxml                    4.9.2\n",
      "Markdown                3.4.4\n",
      "MarkupSafe              2.1.1\n",
      "matplotlib              3.3.0\n",
      "matplotlib-inline       0.1.6\n",
      "mistune                 0.8.4\n",
      "mkl-fft                 1.3.6\n",
      "mkl-random              1.2.2\n",
      "mkl-service             2.4.0\n",
      "munkres                 1.1.4\n",
      "nbclassic               0.5.5\n",
      "nbclient                0.5.13\n",
      "nbconvert               6.4.3\n",
      "nbformat                5.7.0\n",
      "nest-asyncio            1.5.6\n",
      "notebook                6.5.4\n",
      "notebook_shim           0.2.2\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.19.5\n",
      "oauthlib                3.2.2\n",
      "opencv-python           4.8.0.74\n",
      "opt-einsum              3.3.0\n",
      "packaging               23.0\n",
      "pandas                  1.4.4\n",
      "pandas-datareader       0.10.0\n",
      "pandocfilters           1.5.0\n",
      "parso                   0.8.3\n",
      "pickleshare             0.7.5\n",
      "Pillow                  9.4.0\n",
      "pip                     23.2.1\n",
      "pkgutil_resolve_name    1.3.10\n",
      "platformdirs            2.5.2\n",
      "ply                     3.11\n",
      "pooch                   1.4.0\n",
      "prometheus-client       0.14.1\n",
      "prompt-toolkit          3.0.36\n",
      "protobuf                3.20.3\n",
      "psutil                  5.9.0\n",
      "pure-eval               0.2.2\n",
      "pyasn1                  0.5.0\n",
      "pyasn1-modules          0.3.0\n",
      "pycparser               2.21\n",
      "pydot                   1.4.2\n",
      "pydotplus               2.0.2\n",
      "Pygments                2.15.1\n",
      "pyOpenSSL               23.2.0\n",
      "pyparsing               3.0.9\n",
      "pyproj                  3.5.0\n",
      "PyQt5                   5.15.7\n",
      "PyQt5-sip               12.11.0\n",
      "pyrsistent              0.18.0\n",
      "pyshp                   2.3.1\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.8.2\n",
      "python-slugify          8.0.1\n",
      "pytz                    2022.7\n",
      "pywin32                 305.1\n",
      "pywinpty                2.0.10\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.1.0\n",
      "qtconsole               5.4.2\n",
      "QtPy                    2.2.0\n",
      "requests                2.29.0\n",
      "requests-oauthlib       1.3.1\n",
      "rsa                     4.9\n",
      "s3transfer              0.6.0\n",
      "scikit-learn            1.2.2\n",
      "scipy                   1.10.1\n",
      "Send2Trash              1.8.0\n",
      "setuptools              68.0.0\n",
      "sip                     6.6.2\n",
      "six                     1.15.0\n",
      "sniffio                 1.2.0\n",
      "stack-data              0.2.0\n",
      "tensorboard             2.11.2\n",
      "tensorboard-data-server 0.6.1\n",
      "tensorboard-plugin-wit  1.8.1\n",
      "tensorflow              2.4.0\n",
      "tensorflow-addons       0.13.0\n",
      "tensorflow-estimator    2.4.0\n",
      "termcolor               1.1.0\n",
      "terminado               0.17.1\n",
      "testpath                0.6.0\n",
      "text-unidecode          1.3\n",
      "threadpoolctl           2.2.0\n",
      "toml                    0.10.2\n",
      "tornado                 6.2\n",
      "tqdm                    4.65.0\n",
      "traitlets               5.7.1\n",
      "typeguard               4.1.0\n",
      "typing_extensions       4.7.1\n",
      "urllib3                 1.26.16\n",
      "wcwidth                 0.2.5\n",
      "webencodings            0.5.1\n",
      "websocket-client        0.58.0\n",
      "Werkzeug                2.2.3\n",
      "wheel                   0.38.4\n",
      "widgetsnbextension      4.0.5\n",
      "win-inet-pton           1.1.0\n",
      "wrapt                   1.12.1\n",
      "zipp                    3.11.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!nvcc --version\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8475679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7873b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037c56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "def read_yolo_annotation_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    annotations = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(' ')\n",
    "        parts[0] = int(parts[0])\n",
    "        parts[1], parts[2], parts[3], parts[4] = map(float, parts[1:])\n",
    "        annotations.append(parts)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bde08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = \"D:\\Microsoft COCO.v2-raw.yolov8\"\n",
    "train_image_dir = os.path.join(full_data_path,'train','images')\n",
    "train_label_dir = os.path.join(full_data_path, 'train','labels')\n",
    "valid_image_dir = os.path.join(full_data_path,'valid','images')\n",
    "valid_label_dir = os.path.join(full_data_path, 'valid','labels')\n",
    "image_size = 400  # Adjust to your desired image size\n",
    "batch_size = 32\n",
    "patch_size = 100  # Size of the patches to be extracted from the input images\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0255da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "class ObjectDetectionDataLoader(Sequence):\n",
    "    def __init__(self, image_dir, label_dir, image_size, batch_size=32, shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.files = os.listdir(self.image_dir)\n",
    "        self.indices = np.arange(len(self.files))\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_annotations = []\n",
    "        batch_class_labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            file = self.files[idx][:-4]\n",
    "            annotations = read_yolo_annotation_file(os.path.join(self.label_dir, file + '.txt'))\n",
    "            image = load_img(os.path.join(self.image_dir, file + '.jpg'),)\n",
    "            image = image.resize((self.image_size, self.image_size))\n",
    "\n",
    "            for annotation in annotations:\n",
    "                batch_annotations.append(annotation[1:])\n",
    "                batch_images.append(img_to_array(image))\n",
    "                batch_class_labels.append(tf.one_hot(annotation[0], num_classes))  # Adjust based on your annotation format\n",
    "            \n",
    "        return np.array(batch_images), {'bounding_box': np.array(batch_annotations), 'class_predictions': np.array(batch_class_labels)}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_loader = ObjectDetectionDataLoader(train_image_dir, train_label_dir, image_size, batch_size)\n",
    "test_loader = ObjectDetectionDataLoader(valid_image_dir, valid_label_dir, image_size, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc952b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7a01b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    #     Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # return patches\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63971a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    # Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea0f6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(tf.minimum(y_true, y_pred))\n",
    "    union = tf.reduce_sum(tf.maximum(y_true, y_pred))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a6e8e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image, display\n",
    "stop_early_1 = keras.callbacks.EarlyStopping(monitor=\"val_class_predictions_accuracy\", patience=40)\n",
    "stop_early_2 = keras.callbacks.EarlyStopping(monitor=\"val_bounding_box_calculate_iou\", patience=40)\n",
    "def run_experiment(model, batch_size, num_epochs):\n",
    "\n",
    "    # Train the model using both bounding_boxes_y and class_labels_y as y-values.\n",
    "    history = []\n",
    "    with tf.device('/GPU:0'):\n",
    "#        history = model.fit(train_loader, epochs=10, validation_data=test_loader)\n",
    "        history = model.fit(\n",
    "            x = train_loader,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            validation_data=test_loader\n",
    "            #callbacks=[stop_early_1, stop_early_2],\n",
    "        )\n",
    "    \n",
    "    dot_img_file = os.path.join(new_folder, 'architecture.png')\n",
    "    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
    "    display(Image(dot_img_file))\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6046fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build and train\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "num_predictions = 4\n",
    "grid_size = 3\n",
    "\n",
    "def build_model(hp):\n",
    "    input_shape = (image_size, image_size, 3)  # input image shape\n",
    "    learning_rate = hp.Float(\"learning_rate\", 0.0001, 0.01, step=0.00005) # 0.00685\n",
    "    weight_decay = hp.Float(\"weight_decay\", 0.0001, 0.001, step=0.00005) #  0.0002\n",
    "    num_epochs = 100\n",
    "    projection_dim = hp.Int(\"projection_dim\", min_value=30, max_value=64, step=1) # 36\n",
    "    num_heads = hp.Int(\"num_heads\", min_value=1, max_value=32, step=1) # 8\n",
    "    # Size of the transformer layers\n",
    "    transformer_units = [\n",
    "        projection_dim * 2,\n",
    "        projection_dim,\n",
    "    ]\n",
    "\n",
    "    transformer_layers = hp.Int(\"transformer_layers\", min_value=1, max_value=100, step=1) # 58\n",
    "    mlp_head_units = [512, 64, 32]  # Size of the dense layers\n",
    "\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    \n",
    "    \n",
    "    # Add MLP.\n",
    "    #features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "    \n",
    "    #1 units = 704, dropout = 0.1\n",
    "    for i in range(hp.Int(\"num_layers\", min_value=17, max_value=30, step=1)):  # 17\n",
    "        representation = layers.Dense(units=hp.Int(\"num_heads_\" + str(i), min_value=1, max_value=2500, step=50),activation=\"relu\")(representation)\n",
    "        representation = layers.Dropout(hp.Float(\"weight_decay_\" + str(i), 0.001, 0.9, step=0.1))(representation)\n",
    "\n",
    "    # Final MLP head for bounding box prediction\n",
    "    bounding_box = layers.Dense(4, name='bounding_box')(representation)\n",
    "\n",
    "    # Final dense layer for class prediction\n",
    "    class_predictions = layers.Dense(num_classes, activation='softmax', name='class_predictions')(representation)\n",
    "\n",
    "    # Keras model with both bounding box and class predictions\n",
    "    model = keras.Model(inputs=inputs, outputs=[bounding_box, class_predictions])\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Compile the model with appropriate loss functions\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'bounding_box': 'mse', 'class_predictions': 'categorical_crossentropy'},\n",
    "        metrics={'class_predictions': 'accuracy', 'bounding_box': calculate_iou}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "# print('shape of softmax: ' +  str(num_classes))\n",
    "# model = build_model()\n",
    "# # Train model\n",
    "# history = []\n",
    "# history, model = run_experiment(\n",
    "#     model, batch_size, num_epochs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c33403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 40\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': 5e-05, 'sampling': 'linear'}\n",
      "weight_decay (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.001, 'step': 5e-05, 'sampling': 'linear'}\n",
      "projection_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 30, 'max_value': 64, 'step': 1, 'sampling': 'linear'}\n",
      "num_heads (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 32, 'step': 1, 'sampling': 'linear'}\n",
      "transformer_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 100, 'step': 1, 'sampling': 'linear'}\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 17, 'max_value': 30, 'step': 1, 'sampling': 'linear'}\n",
      "num_heads_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_0 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_1 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_2 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_3 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_4 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_5 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_5 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_6 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_6 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_7 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_7 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_8 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_8 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_9 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_9 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_10 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_10 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_11 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_11 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_12 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_12 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_13 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_13 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_14 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_14 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_15 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_15 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n",
      "num_heads_16 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2500, 'step': 50, 'sampling': 'linear'}\n",
      "weight_decay_16 (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "# Instantiate the tuner]\\\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective=[\n",
    "                         kt.Objective(\"val_bounding_box_calculate_iou\", direction=\"max\"),\n",
    "                         kt.Objective(\"val_class_predictions_accuracy\", direction=\"max\"),\n",
    "                               ],\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=1,\n",
    "                     directory=\"D:\\kt_dir\",\n",
    "                     project_name=\"kt_hyperband\",\n",
    "                     overwrite=True)\n",
    "# Display search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22a8e370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 Complete [00h 01m 20s]\n",
      "\n",
      "Best multi_objective So Far: -0.7759705483913422\n",
      "Total elapsed time: 1d 00h 32m 50s\n",
      "\n",
      "Search: Running Trial #18\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.0097            |0.00265           |learning_rate\n",
      "0.00085           |0.0005            |weight_decay\n",
      "45                |57                |projection_dim\n",
      "25                |3                 |num_heads\n",
      "89                |18                |transformer_layers\n",
      "23                |20                |num_layers\n",
      "501               |51                |num_heads_0\n",
      "0.401             |0.301             |weight_decay_0\n",
      "2401              |1551              |num_heads_1\n",
      "0.201             |0.801             |weight_decay_1\n",
      "2301              |351               |num_heads_2\n",
      "0.601             |0.201             |weight_decay_2\n",
      "1351              |1351              |num_heads_3\n",
      "0.101             |0.801             |weight_decay_3\n",
      "601               |2151              |num_heads_4\n",
      "0.501             |0.701             |weight_decay_4\n",
      "201               |2101              |num_heads_5\n",
      "0.601             |0.501             |weight_decay_5\n",
      "2051              |2351              |num_heads_6\n",
      "0.801             |0.001             |weight_decay_6\n",
      "1551              |1801              |num_heads_7\n",
      "0.501             |0.401             |weight_decay_7\n",
      "551               |51                |num_heads_8\n",
      "0.501             |0.801             |weight_decay_8\n",
      "1551              |1301              |num_heads_9\n",
      "0.101             |0.001             |weight_decay_9\n",
      "801               |1801              |num_heads_10\n",
      "0.101             |0.401             |weight_decay_10\n",
      "401               |201               |num_heads_11\n",
      "0.101             |0.501             |weight_decay_11\n",
      "1                 |1501              |num_heads_12\n",
      "0.801             |0.701             |weight_decay_12\n",
      "2351              |251               |num_heads_13\n",
      "0.601             |0.801             |weight_decay_13\n",
      "151               |1851              |num_heads_14\n",
      "0.201             |0.101             |weight_decay_14\n",
      "351               |1351              |num_heads_15\n",
      "0.301             |0.101             |weight_decay_15\n",
      "2001              |251               |num_heads_16\n",
      "0.601             |0.401             |weight_decay_16\n",
      "301               |2251              |num_heads_17\n",
      "0.101             |0.701             |weight_decay_17\n",
      "2101              |2401              |num_heads_18\n",
      "0.501             |0.301             |weight_decay_18\n",
      "1351              |351               |num_heads_19\n",
      "0.601             |0.401             |weight_decay_19\n",
      "1151              |1101              |num_heads_20\n",
      "0.301             |0.801             |weight_decay_20\n",
      "1801              |201               |num_heads_21\n",
      "0.101             |0.601             |weight_decay_21\n",
      "2251              |601               |num_heads_22\n",
      "0.101             |0.801             |weight_decay_22\n",
      "651               |1251              |num_heads_23\n",
      "0.601             |0.201             |weight_decay_23\n",
      "551               |301               |num_heads_24\n",
      "0.501             |0.201             |weight_decay_24\n",
      "2051              |551               |num_heads_25\n",
      "0.001             |0.001             |weight_decay_25\n",
      "1551              |351               |num_heads_26\n",
      "0.601             |0.501             |weight_decay_26\n",
      "2001              |1001              |num_heads_27\n",
      "0.101             |0.701             |weight_decay_27\n",
      "4                 |4                 |tuner/epochs\n",
      "0                 |2                 |tuner/initial_epoch\n",
      "1                 |2                 |tuner/bracket\n",
      "0                 |1                 |tuner/round\n",
      "\n",
      "Epoch 1/4\n",
      "   1/3638 [..............................] - ETA: 59:42:42 - loss: 0.2231 - bounding_box_loss: 0.1977 - class_predictions_loss: 0.0254 - bounding_box_calculate_iou: -0.0023 - class_predictions_accuracy: 0.8472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py\", line 425, in run_trial\n",
      "    return super().run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 214, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 144, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[gradient_tape/model/patch_encoder/embedding/embedding_lookup/Reshape/_2196]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_127793357]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py\", line 425, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 214, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 144, in fit\n    return model.fit(*args, **kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\n    return graph_function._call_flat(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\n    return self._build_call_outputs(self._inference_function.call(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\n    outputs = execute.execute(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model/patch_encoder/embedding/embedding_lookup/Reshape/_2196]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_127793357]\n\nFunction call stack:\ntrain_function -> train_function\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m        \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Your training image data\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#y = {'bounding_box': bounding_boxes_y_train, 'class_predictions': class_labels_y_train},\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_early_2\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:335\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;66;03m# Display needs the updated trial scored by the Oracle.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display\u001b[38;5;241m.\u001b[39mon_trial_end(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id))\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    106\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[1;32m--> 107\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[0;32m    109\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:434\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:386\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures excceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m+\u001b[39m trial\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py\", line 425, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 214, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 144, in fit\n    return model.fit(*args, **kwargs)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\n    return graph_function._call_flat(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\n    return self._build_call_outputs(self._inference_function.call(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\n    outputs = execute.execute(\n  File \"C:\\Users\\EaglesonLabs\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model/patch_encoder/embedding/embedding_lookup/Reshape/_2196]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[4320,1125] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/multi_head_attention_74/query/einsum/Einsum (defined at \\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_127793357]\n\nFunction call stack:\ntrain_function -> train_function\n\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    tuner.search(        \n",
    "        x = train_loader,    # Your training image data\n",
    "        #y = {'bounding_box': bounding_boxes_y_train, 'class_predictions': class_labels_y_train},\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=test_loader,\n",
    "        verbose = 1,\n",
    "        callbacks=[stop_early_1, stop_early_2]\n",
    "                )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "338780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model =  tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1968d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "<class 'keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters'>\n",
      "{'learning_rate': 0.00265, 'weight_decay': 0.0005, 'projection_dim': 57, 'num_heads': 3, 'transformer_layers': 18, 'num_layers': 20, 'num_heads_0': 51, 'weight_decay_0': 0.30100000000000005, 'num_heads_1': 1551, 'weight_decay_1': 0.801, 'num_heads_2': 351, 'weight_decay_2': 0.201, 'num_heads_3': 1351, 'weight_decay_3': 0.801, 'num_heads_4': 2151, 'weight_decay_4': 0.7010000000000001, 'num_heads_5': 2101, 'weight_decay_5': 0.501, 'num_heads_6': 2351, 'weight_decay_6': 0.001, 'num_heads_7': 1801, 'weight_decay_7': 0.401, 'num_heads_8': 51, 'weight_decay_8': 0.801, 'num_heads_9': 1301, 'weight_decay_9': 0.001, 'num_heads_10': 1801, 'weight_decay_10': 0.401, 'num_heads_11': 201, 'weight_decay_11': 0.501, 'num_heads_12': 1501, 'weight_decay_12': 0.7010000000000001, 'num_heads_13': 251, 'weight_decay_13': 0.801, 'num_heads_14': 1851, 'weight_decay_14': 0.101, 'num_heads_15': 1351, 'weight_decay_15': 0.101, 'num_heads_16': 251, 'weight_decay_16': 0.401, 'num_heads_17': 2251, 'weight_decay_17': 0.7010000000000001, 'num_heads_18': 2401, 'weight_decay_18': 0.30100000000000005, 'num_heads_19': 351, 'weight_decay_19': 0.401, 'num_heads_20': 1101, 'weight_decay_20': 0.801, 'num_heads_21': 201, 'weight_decay_21': 0.6010000000000001, 'num_heads_22': 601, 'weight_decay_22': 0.801, 'num_heads_23': 1251, 'weight_decay_23': 0.201, 'num_heads_24': 301, 'weight_decay_24': 0.201, 'num_heads_25': 551, 'weight_decay_25': 0.001, 'num_heads_26': 351, 'weight_decay_26': 0.501, 'num_heads_27': 1001, 'weight_decay_27': 0.7010000000000001, 'tuner/epochs': 4, 'tuner/initial_epoch': 2, 'tuner/bracket': 2, 'tuner/round': 1, 'tuner/trial_id': '0007'}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print('hello')\n",
    "print(type(best_hps))\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c177f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['class_predictions_accuracy'])\n",
    "plt.plot(history.history['val_class_predictions_accuracy'])\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'class predictions accuracy.png')\n",
    "plt.savefig(model_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['bounding_box_loss'])\n",
    "plt.plot(history.history['val_bounding_box_loss'])\n",
    "plt.title('Bounding Box Mean Squared Error')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'bounding box mean squared error.png')\n",
    "plt.savefig(model_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82fd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss during Training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'loss.png')\n",
    "plt.savefig(model_file,dpi=600, facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c890c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['bounding_box_calculate_iou'])\n",
    "plt.plot(history.history['val_bounding_box_calculate_iou'])\n",
    "plt.title('Intersection over Union during Training')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'IoU.png')\n",
    "plt.savefig(model_file,dpi=600, facecolor='w')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5df73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "missed = 0\n",
    "\n",
    "for i in range(0, len(x_test)):\n",
    "    image = x_test[i].numpy().astype(\"uint8\")\n",
    "    # Add an additional dimension for batch size\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    x_center_actual = bounding_boxes_y_test[i][0] * image_size\n",
    "    y_center_actual = bounding_boxes_y_test[i][1] * image_size\n",
    "    width_actual = bounding_boxes_y_test[i][2] * image_size\n",
    "    height_actual = bounding_boxes_y_test[i][3] * image_size\n",
    "\n",
    "    # Calculate x_min, y_min, x_max, and y_max based on the center coordinates, width/heigth and input size\n",
    "    x_min_actual = int(x_center_actual - (width_actual / 2))\n",
    "    y_min_actual = int( y_center_actual - (height_actual / 2))\n",
    "    x_max_actual = x_min_actual + int(width_actual)\n",
    "    y_max_actual = y_min_actual + int(height_actual)\n",
    "\n",
    "    top_left_actual = (x_min_actual, y_min_actual)\n",
    "    bottom_right_actual = (x_max_actual, y_max_actual)\n",
    "\n",
    "    predictions = model(image)\n",
    "\n",
    "\n",
    "    # Extract the bounding box center coordinates from predictions\n",
    "    predicted_box_coords = predictions[0].numpy()[0]\n",
    "    x_center, y_center = predicted_box_coords[0] * image_size,  predicted_box_coords[1] * image_size\n",
    "    width, height = predicted_box_coords[2] * image_size,  predicted_box_coords[3] * image_size\n",
    "\n",
    "    #Extract the prediction label\n",
    "    predicted_labels = predictions[1].numpy()[0]\n",
    "    #print('predicted_labels: ', predicted_labels)\n",
    "    predicted_label =  np.argmax(predicted_labels)\n",
    "    actual_label =  np.argmax(class_labels_y_test[i].numpy())\n",
    "    \n",
    "    # Calculate x_min, y_min, x_max, and y_max based on the center coordinates, width/heigth and input size\n",
    "    x_min = int(x_center - (width / 2))\n",
    "    y_min = int( y_center - (height / 2))\n",
    "    x_max = x_min + int(width)\n",
    "    y_max = y_min + int(height)\n",
    "\n",
    "    top_left = (x_min, y_min)\n",
    "    bottom_right = (x_max, y_max)\n",
    "\n",
    "    # Convert image to RGB if it's grayscale\n",
    "    image = np.squeeze(image)  # Remove the extra batch dimension\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Now use the tuple coordinates in the cv2.rectangle function\n",
    "    color = (0, 255, 0)  # Set green for actual\n",
    "    # Draw rectangles\n",
    "    image = cv2.rectangle(image, top_left_actual, bottom_right_actual, color, 2)\n",
    "    color = (255, 0, 0)  # Set red for prediction\n",
    "    # Draw rectangles\n",
    "    cv2.rectangle(image, top_left, bottom_right, color, 2)\n",
    "    \n",
    "    #only print the cases where the labels disagreed\n",
    "    if actual_label != predicted_label:\n",
    "        missed += 1\n",
    "        print('predicted_box_coords: ', predicted_box_coords)\n",
    "        print('actual_box_coords', bounding_boxes_y_test[i].numpy())\n",
    "        print('predicted_label: ', predicted_label)\n",
    "        print('actual_label: ', actual_label)\n",
    "        # Convert the numpy array to a PIL Image\n",
    "        image_pil = Image.fromarray(image)\n",
    "        # Display the image and make it persist\n",
    "        display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50092e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bb863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
