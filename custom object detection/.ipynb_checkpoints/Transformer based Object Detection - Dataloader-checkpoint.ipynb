{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc449fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.17\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "Package                 Version\n",
      "----------------------- ---------\n",
      "absl-py                 0.15.0\n",
      "anyio                   3.5.0\n",
      "appdirs                 1.4.4\n",
      "argon2-cffi             21.3.0\n",
      "argon2-cffi-bindings    21.2.0\n",
      "asttokens               2.0.5\n",
      "astunparse              1.6.3\n",
      "attrs                   22.1.0\n",
      "Babel                   2.11.0\n",
      "backcall                0.2.0\n",
      "basemap-data            1.3.2\n",
      "bayesian-optimization   1.4.3\n",
      "bleach                  4.1.0\n",
      "boto3                   1.24.28\n",
      "botocore                1.27.59\n",
      "Bottleneck              1.3.5\n",
      "brotlipy                0.7.0\n",
      "cachetools              5.3.1\n",
      "certifi                 2023.7.22\n",
      "cffi                    1.15.1\n",
      "charset-normalizer      2.0.4\n",
      "click                   8.0.4\n",
      "cloudpickle             2.2.1\n",
      "colorama                0.4.6\n",
      "comm                    0.1.2\n",
      "contourpy               1.0.5\n",
      "cryptography            41.0.2\n",
      "cycler                  0.11.0\n",
      "debugpy                 1.5.1\n",
      "decorator               5.1.1\n",
      "defusedxml              0.7.1\n",
      "entrypoints             0.4\n",
      "executing               0.8.3\n",
      "fastjsonschema          2.16.2\n",
      "Flask                   2.2.2\n",
      "flatbuffers             1.12\n",
      "fonttools               4.25.0\n",
      "gast                    0.3.3\n",
      "google-auth             2.22.0\n",
      "google-auth-oauthlib    0.4.6\n",
      "google-pasta            0.2.0\n",
      "grpcio                  1.32.0\n",
      "gym                     0.26.2\n",
      "gym-notices             0.0.8\n",
      "h5py                    2.10.0\n",
      "idna                    3.4\n",
      "importlib-metadata      6.0.0\n",
      "importlib-resources     5.2.0\n",
      "ipykernel               6.19.2\n",
      "ipython                 8.12.0\n",
      "ipython-genutils        0.2.0\n",
      "ipywidgets              8.0.4\n",
      "itsdangerous            2.0.1\n",
      "jedi                    0.18.1\n",
      "Jinja2                  3.1.2\n",
      "jmespath                0.10.0\n",
      "joblib                  1.2.0\n",
      "json5                   0.9.6\n",
      "jsonschema              4.17.3\n",
      "jupyter                 1.0.0\n",
      "jupyter_client          8.1.0\n",
      "jupyter-console         6.6.3\n",
      "jupyter_core            5.3.0\n",
      "jupyter-server          1.13.5\n",
      "jupyterlab              3.3.2\n",
      "jupyterlab-pygments     0.2.2\n",
      "jupyterlab_server       2.16.3\n",
      "jupyterlab-widgets      3.0.5\n",
      "kaggle                  1.5.16\n",
      "Keras-Preprocessing     1.1.2\n",
      "keras-tuner             1.3.5\n",
      "kiwisolver              1.4.4\n",
      "kt-legacy               1.0.5\n",
      "lxml                    4.9.2\n",
      "Markdown                3.4.4\n",
      "MarkupSafe              2.1.1\n",
      "matplotlib              3.3.0\n",
      "matplotlib-inline       0.1.6\n",
      "mistune                 0.8.4\n",
      "mkl-fft                 1.3.6\n",
      "mkl-random              1.2.2\n",
      "mkl-service             2.4.0\n",
      "munkres                 1.1.4\n",
      "nbclassic               0.5.5\n",
      "nbclient                0.5.13\n",
      "nbconvert               6.4.3\n",
      "nbformat                5.7.0\n",
      "nest-asyncio            1.5.6\n",
      "notebook                6.5.4\n",
      "notebook_shim           0.2.2\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.19.5\n",
      "oauthlib                3.2.2\n",
      "opencv-python           4.8.0.74\n",
      "opt-einsum              3.3.0\n",
      "packaging               23.0\n",
      "pandas                  1.4.4\n",
      "pandas-datareader       0.10.0\n",
      "pandocfilters           1.5.0\n",
      "parso                   0.8.3\n",
      "pickleshare             0.7.5\n",
      "Pillow                  9.4.0\n",
      "pip                     23.2.1\n",
      "pkgutil_resolve_name    1.3.10\n",
      "platformdirs            2.5.2\n",
      "ply                     3.11\n",
      "pooch                   1.4.0\n",
      "prometheus-client       0.14.1\n",
      "prompt-toolkit          3.0.36\n",
      "protobuf                3.20.3\n",
      "psutil                  5.9.0\n",
      "pure-eval               0.2.2\n",
      "pyasn1                  0.5.0\n",
      "pyasn1-modules          0.3.0\n",
      "pycparser               2.21\n",
      "pydot                   1.4.2\n",
      "pydotplus               2.0.2\n",
      "Pygments                2.15.1\n",
      "pyOpenSSL               23.2.0\n",
      "pyparsing               3.0.9\n",
      "pyproj                  3.5.0\n",
      "PyQt5                   5.15.7\n",
      "PyQt5-sip               12.11.0\n",
      "pyrsistent              0.18.0\n",
      "pyshp                   2.3.1\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.8.2\n",
      "python-slugify          8.0.1\n",
      "pytz                    2022.7\n",
      "pywin32                 305.1\n",
      "pywinpty                2.0.10\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.1.0\n",
      "qtconsole               5.4.2\n",
      "QtPy                    2.2.0\n",
      "requests                2.29.0\n",
      "requests-oauthlib       1.3.1\n",
      "rsa                     4.9\n",
      "s3transfer              0.6.0\n",
      "scikit-learn            1.2.2\n",
      "scipy                   1.10.1\n",
      "Send2Trash              1.8.0\n",
      "setuptools              68.0.0\n",
      "sip                     6.6.2\n",
      "six                     1.15.0\n",
      "sniffio                 1.2.0\n",
      "stack-data              0.2.0\n",
      "tensorboard             2.11.2\n",
      "tensorboard-data-server 0.6.1\n",
      "tensorboard-plugin-wit  1.8.1\n",
      "tensorflow              2.4.0\n",
      "tensorflow-addons       0.13.0\n",
      "tensorflow-estimator    2.4.0\n",
      "termcolor               1.1.0\n",
      "terminado               0.17.1\n",
      "testpath                0.6.0\n",
      "text-unidecode          1.3\n",
      "threadpoolctl           2.2.0\n",
      "toml                    0.10.2\n",
      "tornado                 6.2\n",
      "tqdm                    4.65.0\n",
      "traitlets               5.7.1\n",
      "typeguard               4.1.0\n",
      "typing_extensions       4.7.1\n",
      "urllib3                 1.26.16\n",
      "wcwidth                 0.2.5\n",
      "webencodings            0.5.1\n",
      "websocket-client        0.58.0\n",
      "Werkzeug                2.2.3\n",
      "wheel                   0.38.4\n",
      "widgetsnbextension      4.0.5\n",
      "win-inet-pton           1.1.0\n",
      "wrapt                   1.12.1\n",
      "zipp                    3.11.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!nvcc --version\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8475679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7873b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28278d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "def read_yolo_annotation_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    annotations = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(' ')\n",
    "        parts[0] = int(parts[0])\n",
    "        parts[1], parts[2], parts[3], parts[4] = map(float, parts[1:])\n",
    "        annotations.append(parts)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd316e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = \"Aero Landing Zone Object Detection.v5i.yolov8\"\n",
    "train_image_dir = os.path.join(full_data_path,'train','images')\n",
    "train_label_dir = os.path.join(full_data_path, 'train','labels')\n",
    "valid_image_dir = os.path.join(full_data_path,'valid','images')\n",
    "valid_label_dir = os.path.join(full_data_path, 'valid','labels')\n",
    "image_size = 400  # Adjust to your desired image size\n",
    "batch_size = 32\n",
    "patch_size = 100  # Size of the patches to be extracted from the input images\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84ef020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "class ObjectDetectionDataLoader(Sequence):\n",
    "    def __init__(self, image_dir, label_dir, image_size, batch_size=32, shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.files = os.listdir(self.image_dir)\n",
    "        self.indices = np.arange(len(self.files))\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_annotations = []\n",
    "        batch_class_labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            file = self.files[idx][:-4]\n",
    "            annotations = read_yolo_annotation_file(os.path.join(self.label_dir, file + '.txt'))\n",
    "            image = load_img(os.path.join(self.image_dir, file + '.jpg'),)\n",
    "            image = image.resize((self.image_size, self.image_size))\n",
    "\n",
    "            for annotation in annotations:\n",
    "                batch_annotations.append(annotation[1:])\n",
    "                batch_images.append(img_to_array(image))\n",
    "                batch_class_labels.append(tf.one_hot(annotation[0], num_classes))  # Adjust based on your annotation format\n",
    "            \n",
    "        return np.array(batch_images), {'bounding_box': np.array(batch_annotations), 'class_predictions': np.array(batch_class_labels)}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_loader = ObjectDetectionDataLoader(train_image_dir, train_label_dir, image_size, batch_size)\n",
    "test_loader = ObjectDetectionDataLoader(valid_image_dir, valid_label_dir, image_size, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc952b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a01b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    #     Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # return patches\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63971a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    # Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0f6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(tf.minimum(y_true, y_pred))\n",
    "    union = tf.reduce_sum(tf.maximum(y_true, y_pred))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6e8e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image, display\n",
    "stop_early_1 = keras.callbacks.EarlyStopping(monitor=\"val_class_predictions_accuracy\", patience=40)\n",
    "stop_early_2 = keras.callbacks.EarlyStopping(monitor=\"val_bounding_box_calculate_iou\", patience=40)\n",
    "def run_experiment(model, batch_size, num_epochs):\n",
    "\n",
    "    # Train the model using both bounding_boxes_y and class_labels_y as y-values.\n",
    "    history = []\n",
    "    with tf.device('/GPU:0'):\n",
    "#        history = model.fit(train_loader, epochs=10, validation_data=test_loader)\n",
    "        history = model.fit(\n",
    "            x = train_loader,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            validation_data=test_loader\n",
    "            #callbacks=[stop_early_1, stop_early_2],\n",
    "        )\n",
    "    \n",
    "    dot_img_file = os.path.join(new_folder, 'architecture.png')\n",
    "    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
    "    display(Image(dot_img_file))\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6046fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of softmax: 80\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    134\u001b[0m history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 135\u001b[0m history, model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(model, batch_size, num_epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#        history = model.fit(train_loader, epochs=10, validation_data=test_loader)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#callbacks=[stop_early_1, stop_early_2],\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     dot_img_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(model, to_file\u001b[38;5;241m=\u001b[39mdot_img_file, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    891\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    892\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#build and train\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "num_predictions = 4\n",
    "grid_size = 3\n",
    "\n",
    "def build_model():\n",
    "    input_shape = (image_size, image_size, 3)  # input image shape\n",
    "    learning_rate = 0.00685 # hp.Float(\"learning_rate\", 0.0001, 0.01, step=0.00005)\n",
    "    weight_decay = 0.0002 # hp.Float(\"weight_decay\", 0.0001, 0.001, step=0.00005)\n",
    "    num_epochs = 100\n",
    "    projection_dim = 36 # hp.Int(\"projection_dim\", min_value=1, max_value=64, step=1)\n",
    "    num_heads = 8 # hp.Int(\"num_heads\", min_value=1, max_value=32, step=1)\n",
    "    # Size of the transformer layers\n",
    "    transformer_units = [\n",
    "        projection_dim * 2,\n",
    "        projection_dim,\n",
    "    ]\n",
    "\n",
    "    transformer_layers = 58 # hp.Int(\"transformer_layers\", min_value=1, max_value=100, step=1)\n",
    "    mlp_head_units = [512, 64, 32]  # Size of the dense layers\n",
    "\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    \n",
    "    \n",
    "    # Add MLP.\n",
    "    #features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "    \n",
    "    #1\n",
    "    representation = layers.Dense(units=704,activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    #2\n",
    "    representation = layers.Dense(units=32, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.6)(representation)\n",
    "    #3\n",
    "    representation = layers.Dense(units=960,activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.4)(representation)\n",
    "    #4\n",
    "    representation = layers.Dense(units=160, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.6)(representation)\n",
    "    #5\n",
    "    representation = layers.Dense(units=352, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    #6\n",
    "    representation = layers.Dense(units=224, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.7)(representation)\n",
    "    #7\n",
    "    representation = layers.Dense(units=480, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.7)(representation)\n",
    "    #8\n",
    "    representation = layers.Dense(units=160, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    #9\n",
    "    representation = layers.Dense(units=736, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.6)(representation)\n",
    "    #10\n",
    "    representation = layers.Dense(units=352, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    #11\n",
    "    representation = layers.Dense(units=320, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    #12\n",
    "    representation = layers.Dense(units=1024, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.4)(representation)\n",
    "    #13\n",
    "    representation = layers.Dense(units=544, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.4)(representation)\n",
    "    #14\n",
    "    representation = layers.Dense(units=384, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.4)(representation)\n",
    "    #15\n",
    "    representation = layers.Dense(units=384, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    #16\n",
    "    representation = layers.Dense(units=896, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    #17\n",
    "    representation = layers.Dense(units=768, activation=\"relu\")(representation)\n",
    "    representation = layers.Dropout(0.7)(representation)\n",
    "    \n",
    "    \n",
    "    # Final MLP head for bounding box prediction\n",
    "    bounding_box = layers.Dense(4, name='bounding_box')(representation)\n",
    "\n",
    "    # Final dense layer for class prediction\n",
    "    class_predictions = layers.Dense(num_classes, activation='softmax', name='class_predictions')(representation)\n",
    "\n",
    "    # Keras model with both bounding box and class predictions\n",
    "    model = keras.Model(inputs=inputs, outputs=[bounding_box, class_predictions])\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Compile the model with appropriate loss functions\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'bounding_box': 'mse', 'class_predictions': 'categorical_crossentropy'},\n",
    "        metrics={'class_predictions': 'accuracy', 'bounding_box': calculate_iou}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "print('shape of softmax: ' +  str(num_classes))\n",
    "model = build_model()\n",
    "# Train model\n",
    "history = []\n",
    "history, model = run_experiment(\n",
    "    model, batch_size, num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c33403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "# Instantiate the tuner]\\\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective=[\n",
    "                         kt.Objective(\"val_bounding_box_calculate_iou\", direction=\"max\"),\n",
    "                         kt.Objective(\"val_class_predictions_accuracy\", direction=\"max\"),\n",
    "                               ],\n",
    "                     max_epochs=100,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=1,\n",
    "                     directory=\"D:\\kt_dir\",\n",
    "                     project_name=\"kt_hyperband\",\n",
    "                     overwrite=True)\n",
    "# Display search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    tuner.search(        \n",
    "        x = x_train,    # Your training image data\n",
    "        y = {'bounding_box': bounding_boxes_y_train, 'class_predictions': class_labels_y_train},\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,\n",
    "        validation_split=0.2,\n",
    "        verbose = 1,\n",
    "        callbacks=[stop_early_1, stop_early_2]\n",
    "                )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model =  tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print('hello')\n",
    "print(type(best_hps))\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c177f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['class_predictions_accuracy'])\n",
    "plt.plot(history.history['val_class_predictions_accuracy'])\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'class predictions accuracy.png')\n",
    "plt.savefig(model_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['bounding_box_loss'])\n",
    "plt.plot(history.history['val_bounding_box_loss'])\n",
    "plt.title('Bounding Box Mean Squared Error')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'bounding box mean squared error.png')\n",
    "plt.savefig(model_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82fd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss during Training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'loss.png')\n",
    "plt.savefig(model_file,dpi=600, facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c890c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['bounding_box_calculate_iou'])\n",
    "plt.plot(history.history['val_bounding_box_calculate_iou'])\n",
    "plt.title('Intersection over Union during Training')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "model_file = os.path.join(new_folder, 'IoU.png')\n",
    "plt.savefig(model_file,dpi=600, facecolor='w')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5df73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "missed = 0\n",
    "\n",
    "for i in range(0, len(x_test)):\n",
    "    image = x_test[i].numpy().astype(\"uint8\")\n",
    "    # Add an additional dimension for batch size\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    x_center_actual = bounding_boxes_y_test[i][0] * image_size\n",
    "    y_center_actual = bounding_boxes_y_test[i][1] * image_size\n",
    "    width_actual = bounding_boxes_y_test[i][2] * image_size\n",
    "    height_actual = bounding_boxes_y_test[i][3] * image_size\n",
    "\n",
    "    # Calculate x_min, y_min, x_max, and y_max based on the center coordinates, width/heigth and input size\n",
    "    x_min_actual = int(x_center_actual - (width_actual / 2))\n",
    "    y_min_actual = int( y_center_actual - (height_actual / 2))\n",
    "    x_max_actual = x_min_actual + int(width_actual)\n",
    "    y_max_actual = y_min_actual + int(height_actual)\n",
    "\n",
    "    top_left_actual = (x_min_actual, y_min_actual)\n",
    "    bottom_right_actual = (x_max_actual, y_max_actual)\n",
    "\n",
    "    predictions = model(image)\n",
    "\n",
    "\n",
    "    # Extract the bounding box center coordinates from predictions\n",
    "    predicted_box_coords = predictions[0].numpy()[0]\n",
    "    x_center, y_center = predicted_box_coords[0] * image_size,  predicted_box_coords[1] * image_size\n",
    "    width, height = predicted_box_coords[2] * image_size,  predicted_box_coords[3] * image_size\n",
    "\n",
    "    #Extract the prediction label\n",
    "    predicted_labels = predictions[1].numpy()[0]\n",
    "    #print('predicted_labels: ', predicted_labels)\n",
    "    predicted_label =  np.argmax(predicted_labels)\n",
    "    actual_label =  np.argmax(class_labels_y_test[i].numpy())\n",
    "    \n",
    "    # Calculate x_min, y_min, x_max, and y_max based on the center coordinates, width/heigth and input size\n",
    "    x_min = int(x_center - (width / 2))\n",
    "    y_min = int( y_center - (height / 2))\n",
    "    x_max = x_min + int(width)\n",
    "    y_max = y_min + int(height)\n",
    "\n",
    "    top_left = (x_min, y_min)\n",
    "    bottom_right = (x_max, y_max)\n",
    "\n",
    "    # Convert image to RGB if it's grayscale\n",
    "    image = np.squeeze(image)  # Remove the extra batch dimension\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Now use the tuple coordinates in the cv2.rectangle function\n",
    "    color = (0, 255, 0)  # Set green for actual\n",
    "    # Draw rectangles\n",
    "    image = cv2.rectangle(image, top_left_actual, bottom_right_actual, color, 2)\n",
    "    color = (255, 0, 0)  # Set red for prediction\n",
    "    # Draw rectangles\n",
    "    cv2.rectangle(image, top_left, bottom_right, color, 2)\n",
    "    \n",
    "    #only print the cases where the labels disagreed\n",
    "    if actual_label != predicted_label:\n",
    "        missed += 1\n",
    "        print('predicted_box_coords: ', predicted_box_coords)\n",
    "        print('actual_box_coords', bounding_boxes_y_test[i].numpy())\n",
    "        print('predicted_label: ', predicted_label)\n",
    "        print('actual_label: ', actual_label)\n",
    "        # Convert the numpy array to a PIL Image\n",
    "        image_pil = Image.fromarray(image)\n",
    "        # Display the image and make it persist\n",
    "        display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50092e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
